{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/aDFH068lHPdBh6QlCge8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tako6969/B4_model-/blob/main/B4_finalmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "これはモデルのみ参考に、その他の操作は適当なコピペ"
      ],
      "metadata": {
        "id": "382F-uSLgKEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jj7PgYaCfuJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import BatchNormalization, add\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import Conv2D, UpSampling2D\n",
        "from keras.layers import add\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dense, Dropout,Input,Average,Conv2DTranspose,SeparableConv2D,dot,UpSampling2D,Add, Flatten,Concatenate,Multiply,Conv2D, MaxPooling2D,Activation,AveragePooling2D, ZeroPadding2D,GlobalAveragePooling2D,multiply,DepthwiseConv2D,ZeroPadding2D,GlobalAveragePooling2D,concatenate ,Lambda\n",
        "\n",
        "\n",
        "kernel_initializer = 'he_uniform'\n",
        "interpolation = \"nearest\"\n",
        "\n",
        "\n",
        "def conv_block_2D(x, filters, block_type, repeat=1, dilation_rate=1, size=3, padding='same'):\n",
        "    result = x\n",
        "\n",
        "    for i in range(0, repeat):\n",
        "\n",
        "        if block_type == 'separated':\n",
        "            result = separated_conv2D_block(result, filters, size=size, padding=padding)\n",
        "        elif block_type == 'duckv2':\n",
        "            result = duckv2_conv2D_block(result, filters, size=size)\n",
        "        elif block_type == 'midscope':\n",
        "            result = midscope_conv2D_block(result, filters)\n",
        "        elif block_type == 'widescope':\n",
        "            result = widescope_conv2D_block(result, filters)\n",
        "        elif block_type == 'resnet':\n",
        "            result = resnet_conv2D_block(result, filters, dilation_rate)\n",
        "        elif block_type == 'conv':\n",
        "            result = Conv2D(filters, (size, size),\n",
        "                            activation='relu', kernel_initializer=kernel_initializer, padding=padding)(result)\n",
        "        elif block_type == 'double_convolution':\n",
        "            result = double_convolution_with_batch_normalization(result, filters, dilation_rate)\n",
        "\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def duckv2_conv2D_block(x, filters, size):\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x1 = widescope_conv2D_block(x, filters)\n",
        "\n",
        "    x2 = midscope_conv2D_block(x, filters)\n",
        "\n",
        "    x3 = conv_block_2D(x, filters, 'resnet', repeat=1)\n",
        "\n",
        "    x4 = conv_block_2D(x, filters, 'resnet', repeat=2)\n",
        "\n",
        "    x5 = conv_block_2D(x, filters, 'resnet', repeat=3)\n",
        "\n",
        "    x6 = separated_conv2D_block(x, filters, size=6, padding='same')\n",
        "\n",
        "    x = add([x1, x2, x3, x4, x5, x6])\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def separated_conv2D_block(x, filters, size=3, padding='same'):\n",
        "    x = Conv2D(filters, (1, size), activation='relu', kernel_initializer=kernel_initializer, padding=padding)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    x = Conv2D(filters, (size, 1), activation='relu', kernel_initializer=kernel_initializer, padding=padding)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def midscope_conv2D_block(x, filters):\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=1)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=2)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def widescope_conv2D_block(x, filters):\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=1)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=2)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=3)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_conv2D_block(x, filters, dilation_rate=1):\n",
        "    x1 = Conv2D(filters, (1, 1), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "                dilation_rate=dilation_rate)(x)\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=dilation_rate)(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=dilation_rate)(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x_final = add([x, x1])\n",
        "\n",
        "    x_final = BatchNormalization(axis=-1)(x_final)\n",
        "\n",
        "    return x_final\n",
        "\n",
        "\n",
        "def double_convolution_with_batch_normalization(x, filters, dilation_rate=1):\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=dilation_rate)(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=dilation_rate)(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def TSFB(x,y,nf1,nf2,gc):\n",
        "\n",
        "    x_ = Conv2D(filters=nf1, kernel_size=3, strides=1,padding='same')(x)\n",
        "    x_ = LeakyReLU(alpha=0.25)(x_)\n",
        "    x_ = BatchNormalization(axis=-1)(x_)\n",
        "\n",
        "    y_ = Conv2D(filters=nf2, kernel_size=3, strides=1,padding='same')(y)\n",
        "    y_ = LeakyReLU(alpha=0.25)(y_)\n",
        "    y_ = BatchNormalization(axis=-1)(y_)\n",
        "\n",
        "    x1 = Conv2D(filters=gc, kernel_size=3, strides=1,padding='same')(x)\n",
        "    x1 = LeakyReLU(alpha=0.25)(x1)\n",
        "    x1 = BatchNormalization(axis=-1)(x1)\n",
        "\n",
        "    y1 = Conv2D(filters=gc, kernel_size=3, strides=1,padding='same')(y)\n",
        "    y1 = LeakyReLU(alpha=0.25)(y1)\n",
        "    y1 = BatchNormalization(axis=-1)(y1)\n",
        "\n",
        "    x1c = Conv2D(filters=gc, kernel_size=3, strides=2,padding='same')(x)\n",
        "    x1c = LeakyReLU(alpha=0.25)(x1c)\n",
        "    x1c = BatchNormalization(axis=-1)(x1c)\n",
        "\n",
        "    y1t = Conv2DTranspose(filters=gc, kernel_size=3, strides=2,padding='same')(y)\n",
        "    y1t = LeakyReLU(alpha=0.25)(y1t)\n",
        "    y1t = BatchNormalization(axis=-1)(y1t)\n",
        "\n",
        "    x2_input = concatenate([x,x1,y1t],axis=-1)\n",
        "    x2 = Conv2D(filters= nf1, kernel_size=3,strides=1, padding='same')(x2_input)\n",
        "    x2 = LeakyReLU(alpha=0.25)(x2)\n",
        "    x2 = BatchNormalization(axis=-1)(x2)\n",
        "\n",
        "    y2_input = concatenate([y,y1,x1c],axis=-1)\n",
        "    y2 = Conv2D(filters= nf2, kernel_size=3,strides=1, padding='same')(y2_input)\n",
        "    y2 = LeakyReLU(alpha=0.25)(y2)\n",
        "    y2 = BatchNormalization(axis=-1)(y2)\n",
        "\n",
        "    x5 = Lambda(lambda x: x * 0.4)(x2)\n",
        "    y5 = Lambda(lambda x: x * 0.4)(y2)\n",
        "\n",
        "    out1 = add([x5,x_])\n",
        "    out2 = add([y5,y_])\n",
        "\n",
        "    out1 = BatchNormalization(axis=-1)(out1)\n",
        "    out2 = BatchNormalization(axis=-1)(out2)\n",
        "\n",
        "    return out1, out2\n",
        "\n",
        "\n",
        "def ChannelAttentionModule(input: tf.keras.Model, ratio=8):\n",
        "    channel = input.shape[-1]\n",
        "\n",
        "    shared_dense_one = tf.keras.layers.Dense(channel // ratio,\n",
        "                                             activation='relu',\n",
        "                                             kernel_initializer='he_normal',\n",
        "                                             use_bias=True,\n",
        "                                             bias_initializer='zeros')\n",
        "    shared_dense_two = tf.keras.layers.Dense(channel,\n",
        "                                             kernel_initializer='he_normal',\n",
        "                                             use_bias=True,\n",
        "                                             bias_initializer='zeros')\n",
        "\n",
        "    avg_pool = tf.keras.layers.GlobalAveragePooling2D()(input)\n",
        "    avg_pool = tf.keras.layers.Reshape((1, 1, channel))(avg_pool)\n",
        "    avg_pool = shared_dense_one(avg_pool)\n",
        "    avg_pool = shared_dense_two(avg_pool)\n",
        "\n",
        "    max_pool = tf.keras.layers.GlobalMaxPooling2D()(input)\n",
        "    max_pool = tf.keras.layers.Reshape((1, 1, channel))(max_pool)\n",
        "    max_pool = shared_dense_one(max_pool)\n",
        "    max_pool = shared_dense_two(max_pool)\n",
        "\n",
        "    x = tf.keras.layers.Add()([avg_pool, max_pool])\n",
        "    x = tf.keras.layers.Activation('sigmoid')(x)\n",
        "\n",
        "    return tf.keras.layers.multiply([input, x])\n",
        "\n",
        "def SpatialAttentionModule(input: tf.keras.Model, kernel_size=7):\n",
        "    avg_pool = tf.keras.layers.Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(input)\n",
        "    max_pool = tf.keras.layers.Lambda(lambda x: K.max(x, axis=3, keepdims=True))(input)\n",
        "    x = tf.keras.layers.Concatenate(axis=3)([avg_pool, max_pool])\n",
        "    for i in [64, 32, 16]:\n",
        "        x = tf.keras.layers.Conv2D(filters=i,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   strides=1,\n",
        "                                   padding='same',\n",
        "                                   activation='relu',\n",
        "                                   kernel_initializer='he_normal',\n",
        "                                   use_bias=False)(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=1,\n",
        "                               kernel_size=kernel_size,\n",
        "                               strides=1,\n",
        "                               padding='same',\n",
        "                               activation='sigmoid',\n",
        "                               kernel_initializer='he_normal',\n",
        "                               use_bias=False)(x)\n",
        "\n",
        "    return tf.keras.layers.multiply([input, x])\n",
        "\n",
        "\n",
        "def CBAM(x):\n",
        "    x = ChannelAttentionModule(x)\n",
        "    x = SpatialAttentionModule(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "H6QU7leafreB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMyvgH-6b8pE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Conv2D, UpSampling2D\n",
        "from keras.layers import add\n",
        "from keras.models import Model\n",
        "\n",
        "kernel_initializer = 'he_uniform'\n",
        "interpolation = \"nearest\"\n",
        "\n",
        "\n",
        "def create_model(img_height, img_width, input_chanels, out_classes, starting_filters):\n",
        "    input_layer = tf.keras.layers.Input((img_height, img_width, input_chanels))\n",
        "\n",
        "    print('Starting DUCK-Net')\n",
        "\n",
        "    p1 = Conv2D(starting_filters * 2, 2, strides=2, padding='same')(input_layer)\n",
        "    p2 = Conv2D(starting_filters * 4, 2, strides=2, padding='same')(p1)\n",
        "    p3 = Conv2D(starting_filters * 8, 2, strides=2, padding='same')(p2)\n",
        "    p4 = Conv2D(starting_filters * 16, 2, strides=2, padding='same')(p3)\n",
        "    p5 = Conv2D(starting_filters * 32, 2, strides=2, padding='same')(p4)\n",
        "\n",
        "    t0_ = conv_block_2D(input_layer, starting_filters, 'duckv2', repeat=1)\n",
        "    t0 =  CBAM(t0_)\n",
        "\n",
        "    l1i = Conv2D(starting_filters * 2, 2, strides=2, padding='same')(t0_)\n",
        "    s1 = add([l1i, p1])\n",
        "    t1 = conv_block_2D(s1, starting_filters * 2, 'duckv2', repeat=1)\n",
        "\n",
        "    l2i = Conv2D(starting_filters * 4, 2, strides=2, padding='same')(t1)\n",
        "    s2 = add([l2i, p2])\n",
        "    t2 = conv_block_2D(s2, starting_filters * 4, 'duckv2', repeat=1)\n",
        "\n",
        "    l3i = Conv2D(starting_filters * 8, 2, strides=2, padding='same')(t2)\n",
        "    s3 = add([l3i, p3])\n",
        "    t3 = conv_block_2D(s3, starting_filters * 8, 'duckv2', repeat=1)\n",
        "\n",
        "    l4i = Conv2D(starting_filters * 16, 2, strides=2, padding='same')(t3)\n",
        "    s4 = add([l4i, p4])\n",
        "    t4 = conv_block_2D(s4, starting_filters * 16, 'duckv2', repeat=1)\n",
        "\n",
        "    l5i = Conv2D(starting_filters * 32, 2, strides=2, padding='same')(t4)\n",
        "    s5 = add([l5i, p5])\n",
        "    t51 = conv_block_2D(s5, starting_filters * 32, 'duckv2', repeat=1)\n",
        "\n",
        "    b1, b2 = TSFB(t0, t1, nf1=17, nf2=34, gc=64)\n",
        "    b4, b3 = TSFB(b2, t2, nf1=34, nf2=68, gc=64)\n",
        "    b5, b6 = TSFB(b1, b4, nf1=17, nf2=34,gc=64)\n",
        "    a2, a1 = TSFB(t4, t51, nf1=272, nf2=544, gc=64)\n",
        "    a4, a3 = TSFB(t3, a2, nf1=136, nf2=272, gc=64)\n",
        "    a6, a5 = TSFB(b3, a4, nf1=68, nf2=136,gc=64)\n",
        "    a8, a7 = TSFB(b6, a6, nf1=34, nf2=68,gc=64)\n",
        "    a10, a9 = TSFB(b5, a8, nf1=17, nf2=34,gc=64)\n",
        "\n",
        "    t53 = conv_block_2D(a1, starting_filters * 16, 'duckv2', repeat=1)\n",
        "\n",
        "    l5o = UpSampling2D((2, 2), interpolation=interpolation)(t53)\n",
        "    c4 = add([l5o, a3])\n",
        "    q4 = conv_block_2D(c4, starting_filters * 8, 'duckv2', repeat=1)\n",
        "\n",
        "    l4o = UpSampling2D((2, 2), interpolation=interpolation)(q4)\n",
        "    c3 = add([l4o, a5])\n",
        "    q3 = conv_block_2D(c3, starting_filters * 4, 'duckv2', repeat=1)\n",
        "\n",
        "    l3o = UpSampling2D((2, 2), interpolation=interpolation)(q3)\n",
        "    c2 = add([l3o, a7])\n",
        "    q6 = conv_block_2D(c2, starting_filters * 2, 'duckv2', repeat=1)\n",
        "\n",
        "    l2o = UpSampling2D((2, 2), interpolation=interpolation)(q6)\n",
        "    c1 = add([l2o, a9])\n",
        "    q1 = conv_block_2D(c1, starting_filters, 'duckv2', repeat=1)\n",
        "\n",
        "    l1o = UpSampling2D((2, 2), interpolation=interpolation)(q1)\n",
        "    c0 = add([l1o, a10])\n",
        "    z1 = conv_block_2D(c0, starting_filters, 'duckv2', repeat=1)\n",
        "    z1_ = conv_block_2D(c0, starting_filters, 'duckv2', repeat=1)\n",
        "\n",
        "    output_1 = conv_block_2D(z1, starting_filters, 'duckv2', repeat=1)\n",
        "    output_2 = conv_block_2D(z1_, starting_filters, 'duckv2', repeat=1)\n",
        "\n",
        "    output_1 = Conv2D(out_classes, (1, 1), activation='sigmoid')(output_1)\n",
        "    output_2 = Conv2D(out_classes, (1, 1), activation='sigmoid')(output_2)\n",
        "\n",
        "    output_1 = Dense(1, activation='sigmoid', name='output_1')(output_1)\n",
        "    output_2 = Dense(1, activation='sigmoid', name='output_2')(output_2)\n",
        "\n",
        "    model = Model(inputs=[input_layer], outputs=[output_1, output_2])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def dice_metric_loss(ground_truth, predictions, smooth=1e-6):\n",
        "    ground_truth = K.cast(ground_truth, tf.float32)\n",
        "    predictions = K.cast(predictions, tf.float32)\n",
        "    ground_truth = K.flatten(ground_truth)\n",
        "    predictions = K.flatten(predictions)\n",
        "    intersection = K.sum(predictions * ground_truth)\n",
        "    union = K.sum(predictions) + K.sum(ground_truth)\n",
        "    dice = (2. * intersection + smooth) / (union + smooth)\n",
        "\n",
        "   #  BCE = K.binary_crossentropy(target=ground_truth, output=predictions)\n",
        "\n",
        "    return 1 - dice\n"
      ],
      "metadata": {
        "id": "Ftg56czrf0MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage.io import imread\n",
        "from tqdm import tqdm\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/new_dataset/\"  # Add the path to your data directory\n",
        "\n",
        "\n",
        "def load_data(img_height, img_width, images_to_be_loaded, dataset):\n",
        "    IMAGES_PATH = folder_path + 'image/'\n",
        "    MASKS_PATH = folder_path + 'lesion mask/'\n",
        "    BONE_PATH = folder_path + 'bone mask/'\n",
        "\n",
        "\n",
        "    if dataset == 'kvasir':\n",
        "        train_ids = glob.glob(IMAGES_PATH + \"*.jpg\")\n",
        "\n",
        "    if dataset == 'cvc-clinicdb':\n",
        "        train_ids = glob.glob(IMAGES_PATH + \"*.tif\")\n",
        "\n",
        "    if dataset == 'cvc-colondb' or dataset == 'etis-laribpolypdb':\n",
        "        train_ids = glob.glob(IMAGES_PATH + \"*.png\")\n",
        "\n",
        "    if images_to_be_loaded == -1:\n",
        "        images_to_be_loaded = len(train_ids)\n",
        "\n",
        "    X_train = np.zeros((images_to_be_loaded, img_height, img_width, 3), dtype=np.float32)\n",
        "    Y_train = np.zeros((images_to_be_loaded, img_height, img_width), dtype=np.uint8)\n",
        "    Z_train = np.zeros((images_to_be_loaded, img_height, img_width), dtype=np.uint8)\n",
        "\n",
        "    print('Resizing training images and masks: ' + str(images_to_be_loaded))\n",
        "    for n, id_ in tqdm(enumerate(train_ids)):\n",
        "        if n == images_to_be_loaded:\n",
        "            break\n",
        "\n",
        "        image_path = id_\n",
        "        mask_path = image_path.replace(\"image\", \"lesion mask\")\n",
        "        bone_path = image_path.replace(\"image\", \"bone mask\")\n",
        "\n",
        "        image = imread(image_path)\n",
        "        mask_ = imread(mask_path)\n",
        "        bone_ = imread(bone_path)\n",
        "\n",
        "        mask = np.zeros((img_height, img_width), dtype=np.bool_)\n",
        "        bone = np.zeros((img_height, img_width), dtype=np.bool_)\n",
        "\n",
        "        pillow_image = Image.fromarray(image)\n",
        "\n",
        "        pillow_image = pillow_image.resize((img_height, img_width))\n",
        "        image = np.array(pillow_image)\n",
        "\n",
        "        X_train[n] = image / 255\n",
        "\n",
        "        pillow_mask = Image.fromarray(mask_)\n",
        "        pillow_mask = pillow_mask.resize((img_height, img_width), resample=Image.LANCZOS)\n",
        "        mask_ = np.array(pillow_mask)\n",
        "\n",
        "        pillow_bone = Image.fromarray(bone_)\n",
        "        pillow_bone = pillow_bone.resize((img_height, img_width), resample=Image.LANCZOS)\n",
        "        bone_ = np.array(pillow_bone)\n",
        "\n",
        "        for i in range(img_height):\n",
        "            for j in range(img_width):\n",
        "                if np.all(mask_[i, j] >= 127):\n",
        "                    mask[i, j] = 1\n",
        "\n",
        "        for i in range(img_height):\n",
        "            for j in range(img_width):\n",
        "                if np.all(bone_[i, j] >= 127):\n",
        "                    bone[i, j] = 1\n",
        "\n",
        "        Y_train[n] = mask\n",
        "\n",
        "        Z_train[n] = bone\n",
        "\n",
        "    Y_train = np.expand_dims(Y_train, axis=-1)\n",
        "\n",
        "    Z_train = np.expand_dims(Z_train, axis=-1)\n",
        "\n",
        "    return X_train, Y_train, Z_train\n",
        "\n",
        "# X_train : Y_train : Z_train = CT : lesion : bone"
      ],
      "metadata": {
        "id": "8Lsex0Ikf0Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import albumentations as albu\n",
        "import numpy as np\n",
        "import gc\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import CSVLogger\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import jaccard_score, precision_score, recall_score, accuracy_score, f1_score\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import keras.backend as K"
      ],
      "metadata": {
        "id": "fvHsTBzFf2Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of GPUs available\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "ktAwrFjef3MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# フィルター17でやってるよ\n",
        "# Setting the model parameters\n",
        "\n",
        "img_size = 224\n",
        "dataset_type = 'BM_Seg' # Options: kvasir/cvc-clinicdb/cvc-colondb/etis-laribpolypdb\n",
        "learning_rate = 0.0001\n",
        "seed_value = 58800\n",
        "filters = 17 # Number of filters, the paper presents the results with 17 and 34\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "ct = datetime.now()\n",
        "\n",
        "model_type = \"DuckNet\"\n",
        "\n",
        "progress_path = '/content/ProgressFull/' + dataset_type + '_progress_csv_' + model_type + '_filters_' + str(filters) +  '_'  + str(ct) + '.csv'\n",
        "progressfull_path = '/content/ProgressFull/' + dataset_type + '_progress_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.txt'\n",
        "plot_path = '/content/ProgressFull/' + dataset_type + '_progress_plot_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.png'\n",
        "model_path = '/content/ModelSaveTensorFlow/' + dataset_type + '/' + model_type + '_filters_' + str(filters) + '_' + str(ct)\n",
        "\n",
        "EPOCHS = 200\n",
        "min_loss_for_saving = 1"
      ],
      "metadata": {
        "id": "-tugSjrjf4oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "\n",
        "X, Y, Z = load_data(img_size, img_size, -1, 'kvasir')"
      ],
      "metadata": {
        "id": "_6rA6YsGf5rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test, z_train, z_test = train_test_split(X, Y, Z, test_size=0.1, shuffle= True, random_state = seed_value)\n",
        "x_train, x_valid, y_train, y_valid, z_train, z_valid = train_test_split(x_train, y_train, z_train, test_size=0.111, shuffle= True, random_state = seed_value)"
      ],
      "metadata": {
        "id": "hqDi0vRWf6t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(152):\n",
        "    print(i)\n",
        "    img_1 = np.array(x_test[i])\n",
        "    img_2 = np.array(y_test[i])\n",
        "    img_3 = np.array(z_test[i])\n",
        "    plt.imshow(img_1)\n",
        "    plt.show()\n",
        "    plt.imshow(img_2)\n",
        "    plt.show()\n",
        "    plt.imshow(img_3)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yXkol1ztf7hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the augmentations\n",
        "\n",
        "aug_train = albu.Compose(\n",
        "    [\n",
        "    albu.HorizontalFlip(p=0.5),\n",
        "    albu.VerticalFlip(p=0.1),\n",
        "    albu.Rotate(limit=35, p=1.0),\n",
        "    ],\n",
        "    additional_targets={'mask': 'image', 'bone': 'image'}\n",
        ")\n",
        "\n",
        "def augment_images():\n",
        "    x_train_out = []\n",
        "    y_train_out = []\n",
        "    z_train_out = []\n",
        "\n",
        "    for i in range (len(x_train)):\n",
        "        ug = aug_train(image=x_train[i], mask=y_train[i], bone=z_train[i])\n",
        "        x_train_out.append(ug['image'])\n",
        "        y_train_out.append(ug['mask'])\n",
        "        z_train_out.append(ug['bone'])\n",
        "\n",
        "    return np.array(x_train_out), np.array(y_train_out), np.array(z_train_out)"
      ],
      "metadata": {
        "id": "9gnrmx8Mf8sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "model = tf.keras.models.load_model(model_path, custom_objects={'dice_metric_loss':dice_metric_loss})\n",
        "\n",
        "ct = datetime.now()\n",
        "model_path = '/content/ModelSaveTensorFlow/' + dataset_type + '/' + model_type + '_filters_' + str(filters) + '_' + str(ct)\n",
        "min_loss_for_saving = 1\n",
        "\n",
        "learning_rate = 0.0005\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss={'output_1':dice_metric_loss, 'output_2':'binary_crossentropy'},loss_weights={'output_1':0.8, 'output_2':0.2})\n",
        "\n",
        "\n",
        "step = 0\n",
        "# 実験\n",
        "\n",
        "# 一旦100でモデルを保存！\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "    if epoch == 50:\n",
        "        learning_rate = 0.0001\n",
        "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "        model.compile(optimizer=optimizer, loss={'output_1':dice_metric_loss, 'output_2':'binary_crossentropy'},loss_weights={'output_1':0.8, 'output_2':0.2})\n",
        "\n",
        "    print(f'Training, epoch {epoch}')\n",
        "    print('Learning Rate: ' + str(learning_rate))\n",
        "\n",
        "    step += 1\n",
        "\n",
        "    image_augmented, mask_augmented, bone_augmented = augment_images()\n",
        "\n",
        "    # csv_logger = CSVLogger(progress_path, append=True, separator=';')\n",
        "    # log_dir = \"logs/\" + str(ct)\n",
        "    # tb_cb = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
        "    log_dir = \"logs/fit/\" + str(ct)\n",
        "    tb_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)\n",
        "\n",
        "    model.fit(x= image_augmented, y={'output_1': mask_augmented, 'output_2': bone_augmented}, epochs=1, batch_size=16, validation_data=(x_valid, {'output_1': y_valid, 'output_2': z_valid}), verbose=1, callbacks=[tb_cb])\n",
        "\n",
        "    prediction_valid, bone_predict_valid = model.predict(x_valid, batch_size=4, verbose=0)\n",
        "    loss_valid = dice_metric_loss(y_valid, prediction_valid)\n",
        "\n",
        "    loss_valid = loss_valid.numpy()\n",
        "    print(\"Loss Validation: \" + str(loss_valid))\n",
        "\n",
        "    dice_valid = f1_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
        "    print(\"dice_valid: \" + str(dice_valid))\n",
        "\n",
        "    # prediction_test = model.predict(x_test, verbose=0)\n",
        "    # loss_test = dice_metric_loss(y_test, prediction_test)\n",
        "    # loss_test = loss_test.numpy()\n",
        "    # print(\"Loss Test: \" + str(loss_test))\n",
        "\n",
        "    # with open(progressfull_path, 'a') as f:\n",
        "        # f.write('epoch: ' + str(epoch) + '\\nval_loss: ' + str(loss_valid) + '\\ntest_loss: ' + str(loss_test) + '\\n\\n\\n')\n",
        "\n",
        "    if min_loss_for_saving > loss_valid:\n",
        "        min_loss_for_saving = loss_valid\n",
        "        print(\"Saved model with val_loss: \", loss_valid)\n",
        "        model.save(model_path)\n",
        "\n",
        "    del image_augmented\n",
        "    del mask_augmented\n",
        "    del bone_augmented\n",
        "\n",
        "    print('---------------------------------------------------------------------------------------------')\n",
        "\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "wtSx_69ogIzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the metrics and saving the results\n",
        "\n",
        "print(\"Loading the model\")\n",
        "model = tf.keras.models.load_model(model_path, custom_objects={'dice_metric_loss':dice_metric_loss})\n",
        "\n",
        "prediction_train, bone_predict_train = model.predict(x_train, batch_size=4)\n",
        "prediction_valid, bone_predict_valid = model.predict(x_valid, batch_size=4)\n",
        "prediction_test, bone_predict_test = model.predict(x_test, batch_size=4)\n",
        "\n",
        "print(\"Predictions done\")\n",
        "\n",
        "dice_train = f1_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_train > 0.5))\n",
        "dice_test = f1_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                          np.ndarray.flatten(prediction_test > 0.5))\n",
        "dice_valid = f1_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Dice finished\")\n",
        "\n",
        "\n",
        "miou_train = jaccard_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_train > 0.5))\n",
        "miou_test = jaccard_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                          np.ndarray.flatten(prediction_test > 0.5))\n",
        "miou_valid = jaccard_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Miou finished\")\n",
        "\n",
        "\n",
        "precision_train = precision_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                                  np.ndarray.flatten(prediction_train > 0.5))\n",
        "precision_test = precision_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                                 np.ndarray.flatten(prediction_test > 0.5))\n",
        "precision_valid = precision_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                                  np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Precision finished\")\n",
        "\n",
        "\n",
        "recall_train = recall_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                            np.ndarray.flatten(prediction_train > 0.5))\n",
        "recall_test = recall_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_test > 0.5))\n",
        "recall_valid = recall_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                            np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Recall finished\")\n",
        "\n",
        "\n",
        "accuracy_train = accuracy_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                                np.ndarray.flatten(prediction_train > 0.5))\n",
        "accuracy_test = accuracy_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                               np.ndarray.flatten(prediction_test > 0.5))\n",
        "accuracy_valid = accuracy_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                                np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "\n",
        "print(\"Accuracy finished\")\n",
        "\n",
        "\n",
        "final_file = 'results_' + model_type + '_' + str(filters) + '_' + dataset_type + '_19_1.txt'\n",
        "print(final_file)\n",
        "\n",
        "with open(final_file, 'a') as f:\n",
        "    f.write(dataset_type + '\\n\\n')\n",
        "    f.write('dice_train: ' + str(dice_train) + ' dice_valid: ' + str(dice_valid) + ' dice_test: ' + str(dice_test) + '\\n\\n')\n",
        "    f.write('miou_train: ' + str(miou_train) + ' miou_valid: ' + str(miou_valid) + ' miou_test: ' + str(miou_test) + '\\n\\n')\n",
        "    f.write('precision_train: ' + str(precision_train) + ' precision_valid: ' + str(precision_valid) + ' precision_test: ' + str(precision_test) + '\\n\\n')\n",
        "    f.write('recall_train: ' + str(recall_train) + ' recall_valid: ' + str(recall_valid) + ' recall_test: ' + str(recall_test) + '\\n\\n')\n",
        "    f.write('accuracy_train: ' + str(accuracy_train) + ' accuracy_valid: ' + str(accuracy_valid) + ' accuracy_test: ' + str(accuracy_test) + '\\n\\n\\n\\n')\n",
        "\n",
        "print('File done')"
      ],
      "metadata": {
        "id": "T_qaqPgvgSvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dice_test)\n",
        "print(miou_test)\n",
        "print(precision_test)\n",
        "print(recall_test)\n",
        "print(accuracy_test)"
      ],
      "metadata": {
        "id": "Gp5QP5vYgT3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_test, bone_predict_test = model.predict(x_test, batch_size=1)"
      ],
      "metadata": {
        "id": "L9qeAvAtgb1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここで一つ一つの精度出せない？"
      ],
      "metadata": {
        "id": "aERztQfEgeiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step=0\n",
        "\n",
        "for i in prediction_test:\n",
        "    print(step)\n",
        "    step+=1\n",
        "    img = np.array(i)\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6xKeIesegdHB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}